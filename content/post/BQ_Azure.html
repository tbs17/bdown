---
title: "How to Query from Google BigQuery and Azure SQL Database at the same time in Python"
author: Tracy Shen
date: 2019-01-21T21:32:20-05:00
slug: Cloud data base connection
categories: ['SQL']
tags: ['Azure cloud database']

---



<p>Google BigQuery and Azure Cloud are both powerful platforms to store data. Google BigQuery can process a couple TB of data within a couple minutes and you pay when you query, store and process. The detailed pricing is <a href="https://cloud.google.com/bigquery/pricing">here</a>.Azure SQL data base provides fast and convienient data for the first 32 GB/month at ~$5/month. The detailed pricing is <a href="https://azure.microsoft.com/en-us/pricing/details/sql-database/managed/">here</a>.Normally business intelligence analysts, database managers or data scientists access the two platforms from the two consoles separately. This leads to that data are scattered and have to be read into a central programming IDE such as RStudio and PyCharm. This post will introduce some quick and dirty way to query data from both Azure database and Google Bigquery in Python using packages that are developed for this purpose.</p>
<div id="connect-to-azure-database-using-pyodbc-package" class="section level3">
<h3>Connect to Azure Database using Pyodbc package</h3>
<p>To query data from Azure SQL database, we will use the package <code>'pyodbc'</code> to connect to the Azure database via the necessary database engine credential.Below I’m taking an example of querying some weather data from an established azure database. The main function we will use is <code>pyodbc.connect()</code></p>
<pre class="python"><code>import os
import pyodbc
import pandas as pd

server = &#39;server name&#39;
database = &#39;database name&#39;
username = &#39;your user name&#39;
password = &#39;your password&#39;
driver= &#39;{ODBC Driver 13 for SQL Server}&#39;

cnxn = pyodbc.connect(&#39;DRIVER=&#39;+driver+&#39;;SERVER=&#39;+server+&#39;;PORT=1433;DATABASE=&#39;+database+&#39;;UID=&#39;+username+&#39;;PWD=&#39;+ password)
</code></pre>
<p>After connecting to the database, if we want to get temperature and precipitation data for the time frame 2017-08-01 to 2018-07-31, we then use the below query and use the function from <code>pandas</code> package <code>pd.read_sql()</code> to convert the data into pandas readable data frame.</p>
<pre class="python"><code>sql1=&#39;&#39;&#39;
    SELECT
[StationID] AS station_id,
[ObservationDate] AS date,
TEMPERATURE_Average AS temp_avg,
TEMPERATURE_Maximum AS temp_max,
TEMPERATURE_Minimum AS temp_min,
[PRECIPITATION_Minimum] AS precip_min,
[PRECIPITATION_Maximum] AS precip_max,
[PRECIPITATION_Average] AS precip_avg,
[TOTAL_DAILY_WATER_EQUIVALENT] AS tot_water
FROM [Aggregated].[StationData_Daily]
WHERE [ObservationDate]&gt;=&#39;2017-08-01&#39; AND [ObservationDate]&lt;&#39;2018-07-31&#39;
AND [StationID] IN (45956,31921,62138)
    &#39;&#39;&#39;
temp_precip1=pd.read_sql(sql1,cnxn)
temp_precip1.head(10)</code></pre>
</div>
<div id="pulling-data-from-google-bigquery" class="section level3">
<h3>Pulling data from google bigquery</h3>
<p>Once you have some temperature and precip data from Azure database, you would want to connect to the Google BigQuery console to download some data from there. In order to do that, we need to install the Python package called <code>'pandas-gbq'</code> and <code>'google.cloud'</code>. Additionally, you would need your project credential information from Google BigQuery(BQ) since everything on BQ is project based and you will also need a private_key which you could only obtain if you have admin access. The private key is a <code>json</code> file which you would need to save under the same directory when connecting to BQ in Python. Please see below code.</p>
<pre class="python"><code>
from google.cloud import bigquery
import pandas_gbq

sql=&quot;&quot;&quot;
select *
FROM `dfp-log-data.DFPdata.Tracy_NYC_365DAY_IMPS`
&quot;&quot;&quot;
# make sure the json file is saved under the same directory of this script
df=pandas_gbq.read_gbq(query=sql,project_id=&#39;xxx&#39;,private_key=&#39;xxx.json&#39;,dialect=&#39;standard&#39;)
df.head(10)
</code></pre>
<p>Connecting to BQ is even easier as it has combined the pandas data frame converting together with the reading bigquery by using a function called <code>pandas_gbq.read_gbq</code>.</p>
<p>Once you have both of the data, you can merge them together and do further analysis on it. There you have it. It is not as difficult as you think. Therefore, there’s no need to put the data from two platforms together and enjoy the benefits of the two platforms at the same time.</p>
</div>
