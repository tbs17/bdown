<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Think Regressively on Think Regressively</title>
    <link>https://thinkregressively.netlify.com/</link>
    <description>Recent content in Think Regressively on Think Regressively</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How Gullible Are You? Predicting Susceptibility to Fake News</title>
      <link>https://thinkregressively.netlify.com/publication/gullible/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 -0400</pubDate>
      
      <guid>https://thinkregressively.netlify.com/publication/gullible/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Query from Google BigQuery and Azure SQL Database at the same time in Python</title>
      <link>https://thinkregressively.netlify.com/post/cloud-data-base-connection/</link>
      <pubDate>Mon, 21 Jan 2019 21:32:20 -0500</pubDate>
      
      <guid>https://thinkregressively.netlify.com/post/cloud-data-base-connection/</guid>
      <description>&lt;p&gt;Google BigQuery and Azure Cloud are both powerful platforms to store data. Google BigQuery can process a couple TB of data within a couple minutes and you pay when you query, store and process. The detailed pricing is &lt;a href=&#34;https://cloud.google.com/bigquery/pricing&#34;&gt;here&lt;/a&gt;.Azure SQL data base provides fast and convienient data for the first 32 GB/month at ~$5/month. The detailed pricing is &lt;a href=&#34;https://azure.microsoft.com/en-us/pricing/details/sql-database/managed/&#34;&gt;here&lt;/a&gt;.Normally business intelligence analysts, database managers or data scientists access the two platforms from the two consoles separately. This leads to that data are scattered and have to be read into a central programming IDE such as RStudio and PyCharm. This post will introduce some quick and dirty way to query data from both Azure database and Google Bigquery in Python using packages that are developed for this purpose.&lt;/p&gt;
&lt;div id=&#34;connect-to-azure-database-using-pyodbc-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Connect to Azure Database using Pyodbc package&lt;/h3&gt;
&lt;p&gt;To query data from Azure SQL database, we will use the package &lt;code&gt;&#39;pyodbc&#39;&lt;/code&gt; to connect to the Azure database via the necessary database engine credential.Below I’m taking an example of querying some weather data from an established azure database. The main function we will use is &lt;code&gt;pyodbc.connect()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import os
import pyodbc
import pandas as pd

server = &amp;#39;server name&amp;#39;
database = &amp;#39;database name&amp;#39;
username = &amp;#39;your user name&amp;#39;
password = &amp;#39;your password&amp;#39;
driver= &amp;#39;{ODBC Driver 13 for SQL Server}&amp;#39;

cnxn = pyodbc.connect(&amp;#39;DRIVER=&amp;#39;+driver+&amp;#39;;SERVER=&amp;#39;+server+&amp;#39;;PORT=1433;DATABASE=&amp;#39;+database+&amp;#39;;UID=&amp;#39;+username+&amp;#39;;PWD=&amp;#39;+ password)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After connecting to the database, if we want to get temperature and precipitation data for the time frame 2017-08-01 to 2018-07-31, we then use the below query and use the function from &lt;code&gt;pandas&lt;/code&gt; package &lt;code&gt;pd.read_sql()&lt;/code&gt; to convert the data into pandas readable data frame.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sql1=&amp;#39;&amp;#39;&amp;#39;
    SELECT
[StationID] AS station_id,
[ObservationDate] AS date,
TEMPERATURE_Average AS temp_avg,
TEMPERATURE_Maximum AS temp_max,
TEMPERATURE_Minimum AS temp_min,
[PRECIPITATION_Minimum] AS precip_min,
[PRECIPITATION_Maximum] AS precip_max,
[PRECIPITATION_Average] AS precip_avg,
[TOTAL_DAILY_WATER_EQUIVALENT] AS tot_water
FROM [Aggregated].[StationData_Daily]
WHERE [ObservationDate]&amp;gt;=&amp;#39;2017-08-01&amp;#39; AND [ObservationDate]&amp;lt;&amp;#39;2018-07-31&amp;#39;
AND [StationID] IN (45956,31921,62138)
    &amp;#39;&amp;#39;&amp;#39;
temp_precip1=pd.read_sql(sql1,cnxn)
temp_precip1.head(10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pulling-data-from-google-bigquery&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pulling data from google bigquery&lt;/h3&gt;
&lt;p&gt;Once you have some temperature and precip data from Azure database, you would want to connect to the Google BigQuery console to download some data from there. In order to do that, we need to install the Python package called &lt;code&gt;&#39;pandas-gbq&#39;&lt;/code&gt; and &lt;code&gt;&#39;google.cloud&#39;&lt;/code&gt;. Additionally, you would need your project credential information from Google BigQuery(BQ) since everything on BQ is project based and you will also need a private_key which you could only obtain if you have admin access. The private key is a &lt;code&gt;json&lt;/code&gt; file which you would need to save under the same directory when connecting to BQ in Python. Please see below code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
from google.cloud import bigquery
import pandas_gbq

sql=&amp;quot;&amp;quot;&amp;quot;
select *
FROM `yourproject.yourtable`
&amp;quot;&amp;quot;&amp;quot;
# make sure the json file is saved under the same directory of this script
df=pandas_gbq.read_gbq(query=sql,project_id=&amp;#39;xxx&amp;#39;,private_key=&amp;#39;xxx.json&amp;#39;,dialect=&amp;#39;standard&amp;#39;)
df.head(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Connecting to BQ is even easier as it has combined the pandas data frame converting together with the reading bigquery by using a function called &lt;code&gt;pandas_gbq.read_gbq&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once you have both of the data, you can merge them together and do further analysis on it. There you have it. It is not as difficult as you think. Therefore, there’s no need to put the data from two platforms together and enjoy the benefits of the two platforms at the same time.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2018 CSSA Grad School Application Seminar</title>
      <link>https://thinkregressively.netlify.com/talk/cssa_talk_2018/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 -0500</pubDate>
      
      <guid>https://thinkregressively.netlify.com/talk/cssa_talk_2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2018 SIA Career Panel</title>
      <link>https://thinkregressively.netlify.com/talk/sia_2018/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 -0500</pubDate>
      
      <guid>https://thinkregressively.netlify.com/talk/sia_2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Run a Tensorflow Model in a Non GPU Supported Computer</title>
      <link>https://thinkregressively.netlify.com/post/tensforflow/</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://thinkregressively.netlify.com/post/tensforflow/</guid>
      <description>&lt;p&gt;Recently, I’ve been playing with the deep learning python package ‘tensorflow’. I ran a simple linear regression model and had some success. Tensorflow is great with unstructured data and image recognization problem. Therefore, it usually runs better in a GPU supported computer. However, given my model is rather simple and won’t need to rely on too much image processing power like GPU. I did it on my windows 7 professional/10 machine and it predicted some values for me. Below are the detailed walk-throughs:&lt;/p&gt;
&lt;div id=&#34;step-1-install-tensorflow-package-into-python&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;step 1: install &lt;code&gt;tensorflow&lt;/code&gt; package into python&lt;/h3&gt;
&lt;p&gt;I use Pycharm- &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34;&gt;a Python IDE&lt;/a&gt; to install tensforflow which is really easy. Go to the settings of Pycharm and navigate to the ‘project interpreter’ tab and click on ‘+’ sign and search ‘tensorflow’ name and click ‘install’ button at the bottom. See below screenshot:&lt;img src=&#34;https://thinkregressively.netlify.com/static/img/install_tensorflow.JPG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-load-a-clean-data-and-split-it-for-training-and-testing-data-using-sklearn&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;step 2: load a clean data and split it for training and testing data using &lt;code&gt;sklearn&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;I’m loading a sample data set called ad_wx and convert some of my columns(feature candidates) to numeric data type for better building features for tensorflow. And you guessed it, &lt;code&gt;&#39;AVG_weedday4&#39;, &#39;AVG_weedday5&#39; and &#39;ad_unit&#39;&lt;/code&gt; will become my 3 features in my simple tensorflow model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
ad_wx=pd.read_csv(&amp;#39;ad-wx.csv&amp;#39;)
ad_wx[&amp;#39;AVG_weedday4&amp;#39;]=ad_wx[&amp;#39;AVG_weedday4&amp;#39;].astype(&amp;#39;int64&amp;#39;)
ad_wx[&amp;#39;AVG_weedday5&amp;#39;]=ad_wx[&amp;#39;AVG_weedday5&amp;#39;].astype(&amp;#39;int64&amp;#39;)
ad_wx[&amp;#39;ad_unit&amp;#39;]=ad_wx[&amp;#39;ad_unit&amp;#39;].astype(&amp;#39;str&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I use sklearn.model_selection to split the data into training and testing dataset with testing dataset to be 30% of total data. &lt;code&gt;clicks/impression&lt;/code&gt; is my target variable or predicting variable.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split
x_data=ad_wx.drop(&amp;#39;clicks/impression&amp;#39;,axis=1)
y_data=ad_wx[&amp;#39;clicks/impression&amp;#39;]
X_train,X_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.3,random_state=100)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-build-feature-column-and-input-function-using-tensorflow-estimator-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;step 3: build feature column and input function using &lt;code&gt;tensorflow estimator API&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;After we had training and testing data, we are ready to run the tensorflow model. To build a model in Tensorflow, we basically need to build a &lt;code&gt;feature column&lt;/code&gt; list and &lt;code&gt;input function&lt;/code&gt; to train the model. For features to be tensorflow ready, we need to convert them into the datatype that tensorflow can recognize. There are two functions that are commonly used to convert features. &lt;code&gt;categorical_column_with_has_bucket()&lt;/code&gt; and &lt;code&gt;numeric_column()&lt;/code&gt; are used to convert catgorical variable to tensorflow ready feature as well as numeric variables. Once done, you can throw them into a list.&lt;/p&gt;
&lt;p&gt;Regarding input function, you use your training data to train that input function by batch loadin them (see batch_size argument below) and iterate for n times (see num_epochs argument) with shuffling.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
ad_unit=tf.feature_column.categorical_column_with_hash_bucket(&amp;#39;ad_unit&amp;#39;,hash_bucket_size=1000)
weed4=tf.feature_column.numeric_column(&amp;#39;AVG_weedday4&amp;#39;)
weed5=tf.feature_column.numeric_column(&amp;#39;AVG_weedday5&amp;#39;)
feat_col=[ad_unit,weed4,weed5]
input_func=tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,batch_size=10,num_epochs=100,shuffle=True)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-predict-target-values-using-cpu-processing-patch&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;step 4: predict target values using CPU processing patch&lt;/h3&gt;
&lt;p&gt;After you are done with building the essential parts for the model, you can then build the model and train the data. After the model is built and trained, then you can use a prediction function to predict the target variables. prediction function is basically an input function that uses testing data. You use &lt;code&gt;model.predict()&lt;/code&gt; to predict values. The output predicted numbers initially is a dictionary. You will need to convert it to a list so that you can extract the predicted value. After that’s done, you can write it out using numpy’s &lt;code&gt;savetxt()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model=tf.estimator.LinearRegressor(feature_columns=feat_col)
model.train(input_fn=input_func,steps=500)
pred_fn=tf.estimator.inputs.pandas_input_fn(x=X_test,batch_size=len(X_test),num_epochs=100,shuffle=False)
predictions=list(model.predict(input_fn=pred_fn))
final_preds=[]
for pred in predictions:
    final_preds.append(pred[&amp;#39;predictions&amp;#39;][0])
    
import numpy as np
np.savetxt(&amp;#39;tf_predictions.csv&amp;#39;,final_preds,delimiter=&amp;#39;,&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All the above steps are easy and simple except for the last part.To get the tensorflow model trained and render the predicted number, you usually need a GPU supported computer to run. However, there’s a work-around way for you to just use CPU computer to finish running the model.&lt;/p&gt;
&lt;p&gt;When you run your model, you will see the below error message:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;2018-07-13 09:01:33.900329: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the solution is to update the tensorflow binary for your CPU &amp;amp; OS using a wheel file. To install, use the below command in your &lt;code&gt;command line prompt&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install --ignore-installed --upgrade &#39;Download whl file&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The download url of the whl file can be found &lt;a href=&#34;https://github.com/lakshayg/tensorflow-build&#34;&gt;here&lt;/a&gt;. For windows, you will want to use &lt;a href=&#34;https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.8.0/py36/CPU/avx2&#34;&gt;this wheel file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After you download it, save it under your default command line directory. Mine is ‘C:.shen’ and install it into your machine. Then you are good to go.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Memory Illusion</title>
      <link>https://thinkregressively.netlify.com/project/memory_illusion/</link>
      <pubDate>Mon, 09 Jul 2018 10:00:00 -0400</pubDate>
      
      <guid>https://thinkregressively.netlify.com/project/memory_illusion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SysFake</title>
      <link>https://thinkregressively.netlify.com/project/user_susceptibility/</link>
      <pubDate>Sun, 01 Jul 2018 10:00:00 -0400</pubDate>
      
      <guid>https://thinkregressively.netlify.com/project/user_susceptibility/</guid>
      <description></description>
    </item>
    
    <item>
      <title>My Journey to Data Science</title>
      <link>https://thinkregressively.netlify.com/post/journey/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://thinkregressively.netlify.com/post/journey/</guid>
      <description>&lt;p&gt;If you have viewed my bio, you probably noticed that I don’t have a science or engineering background, so how come I end up with data scientist? Well, it turns out that you don’t need a science or engineering background to become a data scientist! Here’s my personal trajectory. I think it’s highly reproducible:)&lt;/p&gt;
&lt;p&gt;In Sep 2014, I was hired as a marketing research analyst by AccuWeather to work on AW’s new adventure: IoT or we call it emerging platform projects. The mission of the position is to assist the team to build weather based algorithm to power digital health, smart home and connected car apps/widgets. To complete the mission, we need data scientists and I was tasked to research on everything about data scientist as a job. From there, I found the below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data scientists got paid really well: above 11k annual salary&lt;/li&gt;
&lt;li&gt;data scientist job normally requires statistical, coding skills&lt;/li&gt;
&lt;li&gt;data scientist is on high demand&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These findings triggered ‘Aha’ moment for me: I WANT TO BE A DATA SCIENTIST! But how?! At that time, there isn’t any data science major at any regular university. From what I’ve researched, almost all the data scientists are made into instead of graduated from. Therefore, I think there must a kit that contains everything that one needs to become a data scientist. Through coursera(MOOC website), I found Johns Hopkins Data Science Specialization course. It’s basically a 10-course specialization and not only teaches coding but also statistics. You can find more details &lt;a href=&#34;https://www.coursera.org/specializations/jhu-data-science&#34;&gt;here&lt;/a&gt;. I was so exited and fully sold and for the first time I spent money on some online course. Although the money later on got reimbursed by work, I was so determined that I think it will be a great investment. It turns out to be true because that way I was super movitated to finish a course each month and I also obtained a certificate from all the hard work I’ve done. To me, if I pay for something, I want to get my money worth. So I’m kind of person who only goes to buffet restaurant if I’m super hungry,lol.&lt;/p&gt;
&lt;p&gt;The Johns Hopkins Data Science specailization not only taught me R but also opened the door to a programming and statistical world. From there, I got to know all these stellar statistians/data scientists such as my instructor Roger Peng, Brian Caffo, Jeff Leek. From there, I started to pay attention to data science related podcasts, such as ‘Not So Standard Deviation(NSSD)’. From NSSD, I got to know Hilary Parker, a data scientist from StitchFix. From Roger Peng, I got to know ‘Effort Report’ a podcast that focuses on academia life. From NSSD,I got to know that Python is another major lanaguage that is acknlowledged unanimously among data science field. I then enrolled myself intot a 5-course python specialization taught by University of Michigan &lt;a href=&#34;https://www.coursera.org/specializations/data-science-python&#34;&gt;‘Applied Data Science in Python’&lt;/a&gt;.From there, I got to know a python focused podcast ‘Data Skeptics’. For a very long time, NSSD and DS are the only two data science related podcasts that kept me company and inspired me and pulled me closer and closer to the data science field, although nowadays I subscribed to many other data science and machine learning podcasts(see the list at the end). Podcasts are great to keep up with the latest news in the field and listen to the past expeirence and successful stories of varieties of data scientists as well as the data science projects.&lt;/p&gt;
&lt;p&gt;During this process, my title has changed from marketing research analyst to business intelligence analyst and to data scientist. At the same time, I’ve also served as an editor to a R newsletter called &lt;a href=&#34;https://rweekly.org/&#34;&gt;RWeekly&lt;/a&gt; and the social media chair of &lt;a href=&#34;http://forwards.github.io/&#34;&gt;Forewards&lt;/a&gt; where R minority users get together to promote R user in under-represented groups.Of course, taking online courses is not enough. I’ve also worked on a couple data science projects using R and Python at work. Some of them are statistical models such as logistic regression, regular regression with seasonality factors. Some of them are just data cleaning tasks which nowadays is a big part of data science, taking about 70-80% of data scientists’ time. At the moment of writing this blog, I’m one month away from my PhD semester begins. Yes, the journey leads to another wonderful world and hopefully you will see another post about my data science experience in academia later.&lt;/p&gt;
&lt;p&gt;List of data science related podcast:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not So Standard Deviation: by Roger Peng and Hilary Parker&lt;/li&gt;
&lt;li&gt;Data Skeptic: by Kyle Polich&lt;/li&gt;
&lt;li&gt;DataFramed: Data Camp podcast&lt;/li&gt;
&lt;li&gt;Super Data Science: hosted by Kirrill Erimenko-Data Scientist and Lifestyle Entrepreneur&lt;/li&gt;
&lt;li&gt;Linear Digressions: by Ben Jaffe and Katie Malone&lt;/li&gt;
&lt;li&gt;This Week in Machine Learning and Artificial Intelligence&lt;/li&gt;
&lt;li&gt;Learning Machines 101: by Richard Golden&lt;/li&gt;
&lt;li&gt;Data Stories: Enrico Bertini and Moritz Stefaner&lt;/li&gt;
&lt;li&gt;Google Cloud Platform Podcast&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2016 SIA Career Panel</title>
      <link>https://thinkregressively.netlify.com/talk/sia_talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://thinkregressively.netlify.com/talk/sia_talk/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
